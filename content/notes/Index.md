---
longform:
  format: scenes
  title: notes
  workflow: Default Workflow
  sceneFolder: /
  scenes:
    - Residual Connection
    - Layer Norm
    - RNN
    - Transformer
    - Large Language Model
    - Relative Positional Encoding
    - KV cache
    - Multi-Query Attention
    - LLaMA
    - Rotary Position Embeddings (RoPE)
    - Attention with Linear Biases (ALiBi)
    - Auto regressive decoding
    - Batch Norm
    - Euler's Formula
    - GPUs and Other Accelerators
    - Gated Linear Unit (GLU)
    - Group-Query Attention
    - Decision Tree
    - Gradient Descent
    - Perceptron
    - neuron networks
    - kernels
    - SVM
    - Backpropagation
    - Deep Learning
    - Adam
    - AdamW
    - Convolution
    - Convolutional networks
    - LLaMA 2
    - LLaMA 3.1
  sceneTemplate: templates/Chapter.md
  ignoredFiles:
    - normalization methods
draft: false
---
